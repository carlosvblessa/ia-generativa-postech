{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuymKShJDtPK"
   },
   "source": [
    "## Vamos implementar um exemplo simples de Self-Attention em Python\n",
    "\n",
    "Este notebook mostra, em pequenos blocos, como construímos todas as peças necessárias para o mecanismo de self-attention: começamos representando palavras como vetores, definimos as funções auxiliares (softmax e multiplicações matriciais) e, por fim, avaliamos como cada token passa a enxergar os demais dentro da mesma frase. O objetivo é manter a matemática acessível, destacando as etapas fundamentais antes de avançar para arquiteturas maiores como Transformers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Passos iniciais**\n",
    "1. **Transformar as palavras em vetores numéricos (One-Hot Encoding)** — convertemos cada palavra para um vetor binário para que a frase possa ser manipulada com álgebra linear.\n",
    "2. **Aplicar o mecanismo de Self-Attention ao resultado** — calculamos as matrizes *queries*, *keys* e *values*, estimamos quanto cada palavra deve prestar atenção às demais e geramos uma nova representação contextualizada.\n",
    "3. **Exibir e interpretar a saída** — imprimimos os vetores finais para observar como a atenção redistribuiu a importância entre os tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "A codificação one-hot transforma cada palavra em um vetor cujo tamanho é igual ao vocabulário e onde apenas uma posição recebe o valor 1. Essa abordagem simples evita ambiguidade (palavras diferentes nunca compartilham o mesmo vetor) e cria uma base neutra para os próximos cálculos. Apesar de não capturar semântica por si só, ela facilita demonstrar o comportamento da atenção sem depender de embeddings pré-treinados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot_encode(word, vocab):\n",
    "    vector = np.zeros(len(vocab))\n",
    "    vector[vocab.index(word)] = 1\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exemplo prático**: considere o vocabulário `['o', 'gato', 'pulou', 'do', 'muro']`. A palavra `gato` ocupa a segunda posição dessa lista, logo seu vetor one-hot torna-se `[0, 1, 0, 0, 0]`. Já `muro` seria `[0, 0, 0, 0, 1]`. Como todos os vetores possuem o mesmo tamanho, conseguimos empilhá-los em uma matriz para representar a frase inteira.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"o\", \"gato\", \"pulou\", \"no\", \"telhado\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vocabulário e ordem**: o dicionário de palavras únicas precisa ser montado antes da codificação e, principalmente, mantido na mesma ordem sempre que reutilizarmos o modelo. Isso garante que o índice da palavra corresponda à mesma coluna do vetor em todas as etapas. Em cenários reais, o vocabulário pode vir de uma lista global, de *tokenizers* treinados ou de subpalavras (BPE, WordPiece).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(dict.fromkeys(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGBU0Kj6D-72"
   },
   "source": [
    "A função softmax converte as pontuações (que podem assumir qualquer valor real) em probabilidades normalizadas, destacando as palavras mais relevantes sem descartar completamente as demais. Ao dividir pelo somatório exponencial, garantimos que todos os pesos fiquem entre 0 e 1 e que a soma seja exatamente 1, condição ideal para interpretar a atenção como uma distribuição de importância.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BAK5mVoADkZL"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `self_attention` calcula o quanto cada token deve focar em si mesmo e nos outros tokens. Ela implementa os passos clássicos: gera as matrizes de *queries*, *keys* e *values*, mede similaridades por produtos escalares e reescala os valores pelo softmax para produzir uma combinação ponderada que carrega contexto global da frase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6EpoH-U5D0FJ"
   },
   "outputs": [],
   "source": [
    "def self_attention(Q, K, V):\n",
    "    # Calcula as pontuações de atenção\n",
    "    scores = np.dot(Q, K.T)\n",
    "    # Aplica softmax às pontuações\n",
    "    attention_weights = softmax(scores)\n",
    "    # Calcula a saída ponderada\n",
    "    output = np.dot(attention_weights, V)\n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9UaqJeHEMLE"
   },
   "source": [
    "- `scores`: resultados do produto `Q @ K^T`, que mede a compatibilidade entre cada par de palavras; quanto maior o score, maior a afinidade.\n",
    "- `attention_weights`: aplica `softmax` em cada linha de `scores` para obter pesos normalizados, interpretados como \"porcentagens de atenção\" destinadas aos demais tokens.\n",
    "- `output`: multiplica os pesos pelos vetores `V`, produzindo uma nova representação em que cada palavra incorpora informações das palavras que recebeu maior atenção.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([one_hot_encode(word,vocab) for word in sentence])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aplicando Self-Attention\n",
    "output, attention_weights = self_attention(X, X, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aYprMFZEP9E"
   },
   "source": [
    "A matriz de entrada é composta pela pilha dos vetores one-hot (ou embeddings) de cada palavra, com dimensões `(n_palavras, tamanho_vocab)`. Cada linha descreve um token e cada coluna corresponde a um item do vocabulário. Esse formato facilita aplicar multiplicações matriciais em lote, o que é fundamental para que arquiteturas de atenção sejam eficientes em GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6duTNvMLD2-a",
    "outputId": "70704d59-c9ed-496d-ad0e-f54d4d065b14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário\n",
      "['o', 'gato', 'pulou', 'no', 'telhado']\n",
      "Entrada (X):\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "\n",
      "Pesos de Atenção:\n",
      "[[0.40460968 0.14884758 0.14884758 0.14884758 0.14884758]\n",
      " [0.14884758 0.40460968 0.14884758 0.14884758 0.14884758]\n",
      " [0.14884758 0.14884758 0.40460968 0.14884758 0.14884758]\n",
      " [0.14884758 0.14884758 0.14884758 0.40460968 0.14884758]\n",
      " [0.14884758 0.14884758 0.14884758 0.14884758 0.40460968]]\n",
      "\n",
      "Saída com Self-Attention Aplicada:\n",
      "[[0.40460968 0.14884758 0.14884758 0.14884758 0.14884758]\n",
      " [0.14884758 0.40460968 0.14884758 0.14884758 0.14884758]\n",
      " [0.14884758 0.14884758 0.40460968 0.14884758 0.14884758]\n",
      " [0.14884758 0.14884758 0.14884758 0.40460968 0.14884758]\n",
      " [0.14884758 0.14884758 0.14884758 0.14884758 0.40460968]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulário\")\n",
    "print(vocab)\n",
    "print(\"Entrada (X):\")\n",
    "print(X)\n",
    "print(\"\\nPesos de Atenção:\")\n",
    "print(attention_weights)\n",
    "print(\"\\nSaída com Self-Attention Aplicada:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste código, estamos manualmente definindo embeddings para palavras, atribuindo-lhes valores que indicam suas relações semânticas simplificadas. Esses embeddings ajudam o modelo a entender o contexto e a importância das palavras na frase ao aplicar mecanismos como o Self-Attention. Em aplicações reais, esses vetores são aprendidos automaticamente a partir de grandes conjuntos de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentença: \n",
      "['o', 'gato', 'pulou', 'no', 'telhado']\n",
      "\n",
      "Pesos de Atenção: \n",
      "[[0.28625735 0.23436769 0.12862372 0.11638355 0.23436769]\n",
      " [0.25887999 0.22505945 0.15086186 0.13787736 0.22732134]\n",
      " [0.16980847 0.18030884 0.28562254 0.18030884 0.18395132]\n",
      " [0.16237652 0.17415015 0.19055061 0.28426811 0.1886546 ]\n",
      " [0.25345973 0.22256183 0.15068702 0.14623354 0.22705788]]\n",
      "\n",
      "Saída com Self-Attention Aplicada:\n",
      "[[0.69860875 0.16141087 0.18914189]\n",
      " [0.66474473 0.1797153  0.20844447]\n",
      " [0.53637198 0.28295493 0.25619273]\n",
      " [0.51915726 0.21714778 0.32067055]\n",
      " [0.65791626 0.18013494 0.214792  ]]\n"
     ]
    }
   ],
   "source": [
    "embeddings = {\n",
    "    'o': np.array([1., 0., 0.]),\n",
    "    'gato': np.array([0.8, 0.1, 0.1]),\n",
    "    'pulou': np.array([0.2, 0.8, 0.2]),\n",
    "    'no': np.array([0.1, 0.1, 0.8]),\n",
    "    'telhado': np.array([0.8, 0.1, 0.2])\n",
    "}\n",
    "\n",
    "X = np.array([embeddings[word] for word in sentence])\n",
    "\n",
    "output, attention_weights = self_attention(X, X, X)\n",
    "\n",
    "print(\"Sentença: \")\n",
    "print(sentence)\n",
    "print(\"\\nPesos de Atenção: \")\n",
    "print(attention_weights)\n",
    "print(\"\\nSaída com Self-Attention Aplicada:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando os pesos de atenção\n",
    "A matriz acima mostra quanto cada palavra foca nas demais. Observando as maiores probabilidades em cada linha:\n",
    "- `o` distribui atenção de forma relativamente uniforme, mas foca ligeiramente em `gato` e `telhado` (≈0.23).\n",
    "- `gato` dá mais peso a `o` (≈0.26), mantendo relevância parecida para `telhado`.\n",
    "- `pulou` prioriza a si próprio (≈0.29).\n",
    "- `no` direciona a maior parte para si (≈0.28).\n",
    "- `telhado` distribui atenção entre `o` e `gato`, reforçando o sujeito da frase.\n",
    "\n",
    "No geral, os maiores pesos aparecem nas diagonais (palavra prestando atenção em si mesma) e nos pares (`o`↔`gato`, `telhado`↔`gato`), indicando que o modelo considera sujeito e local como elementos-chave para representar o contexto dessa sentença."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP3X6ZcdsYr4pmZ0d6vVXxi",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
