{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoaL07m8sCMV"
      },
      "source": [
        "Nesta parte da aula, vamos configurar o ambiente de desenvolvimento necessário para criar e treinar Redes Generativas Adversariais (GANs). Vamos usar o Google Colab para isso, e vamos instalar as bibliotecas essenciais, como TensorFlow e Keras.\n",
        "\n",
        "O Google Colab é uma excelente escolha, pois oferece um ambiente configurado na nuvem com suporte a GPUs, o que é muito útil para treinar modelos de deep learning como GANs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9BhyhUVseV4",
        "outputId": "b7f48aff-afc1-4734-ff1d-673183499622"
      },
      "outputs": [],
      "source": [
        "# Instalando TensorFlow no Google Colab (caso necessário)\n",
        "# !pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "phQeIvWF-VOJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-13 18:13:18.559672: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-13 18:13:18.593788: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-13 18:13:19.396886: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RDC5X58sTR4"
      },
      "source": [
        "### Verificando se temos acesso a uma GPU\n",
        "O que é uma GPU? Uma GPU (Graphics Processing Unit) é um tipo especial de processador que é muito bom em fazer cálculos rápidos, especialmente para tarefas que envolvem muitos dados, como treinar redes neurais.\n",
        "\n",
        "Por que isso é importante? Quando você está treinando um modelo de inteligência artificial (como o que estamos fazendo aqui), pode demorar muito tempo se você usar apenas o processador comum do seu computador (a CPU). Usar uma GPU pode acelerar esse processo, às vezes tornando-o dezenas de vezes mais rápido.\n",
        "\n",
        "O que o código faz? Esse pedaço de código verifica se o ambiente na nuvem, como o Google Colab tem uma GPU disponível que possamos usar. Ele simplesmente conta quantas GPUs estão disponíveis e imprime esse número. Se o resultado for zero, significa que estamos usando apenas a CPU, o que pode ser mais lento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s4VmczstAOB",
        "outputId": "1af7f5d9-4d21-4b4c-cf6e-65a0a573b339"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-13 18:13:19.987380: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0ak-kjltHV_"
      },
      "source": [
        "### Importando o conjunto de dados MNIST\n",
        "\n",
        "O que é o MNIST? MNIST é um conjunto de dados muito famoso na área de inteligência artificial. Ele contém 70.000 imagens de dígitos escritos à mão (números de 0 a 9). Cada imagem é pequena, com 28 pixels de largura e 28 pixels de altura, e é em preto e branco.\n",
        "\n",
        "Por que usar o MNIST? Como essas imagens são pequenas e simples, o MNIST é frequentemente usado para testar e treinar modelos de inteligência artificial, especialmente para quem está começando. É como um \"caderno de exercícios\" para aprender a fazer o computador reconhecer padrões em imagens.\n",
        "\n",
        "O que o código faz? Aqui, estamos \"baixando\" essas imagens para o nosso programa. A função tf.keras.datasets.mnist.load_data() carrega as imagens do conjunto de dados MNIST. Nós dividimos esses dados em duas partes: train_images, que são as imagens que usaremos para treinar nosso modelo, e o resto (os rótulos, que identificam quais números são mostrados em cada imagem), que não usaremos por enquanto, por isso os ignoramos com _."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BRxjij_9tS_P"
      },
      "outputs": [],
      "source": [
        "(train_images, _), (_, _) = tf.keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA4UCMqrtecG"
      },
      "source": [
        "### Normalizando as imagens para o intervalo [-1, 1]\n",
        "\n",
        "O que é normalização? Normalização é uma técnica usada para ajustar os valores dos dados para que fiquem dentro de um intervalo específico. Isso ajuda o modelo de inteligência artificial a processar os dados de maneira mais eficiente.\n",
        "\n",
        "Por que normalizar as imagens? As imagens do MNIST são compostas por pixels, e cada pixel tem um valor que vai de 0 a 255 (onde 0 é preto e 255 é branco). Normalizamos esses valores para que fiquem entre -1 e 1, o que facilita o trabalho da rede neural, tornando o aprendizado mais rápido e estável.\n",
        "\n",
        "O que o código faz?\n",
        "\n",
        "Reshape: A linha train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32') ajusta as imagens para que tenham a forma correta de [28x28 pixels] e uma única camada de cor (já que são imagens em preto e branco). Além disso, converte os valores para o tipo float32, que é um formato de número mais adequado para cálculos precisos.\n",
        "\n",
        "Normalização: A linha train_images = (train_images - 127.5) / 127.5 normaliza os valores dos pixels. Subtrai-se 127,5 de cada valor de pixel para centralizar os valores em torno de 0 e, em seguida, divide-se por 127,5 para que os valores fiquem no intervalo de -1 a 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aXTrrWhmtbHU"
      },
      "outputs": [],
      "source": [
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "train_images = (train_images - 127.5) / 127.5  # Normalizando para o intervalo [-1, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKO3FA2Gtxox"
      },
      "source": [
        "### Configurando o buffer e o batch size\n",
        "\n",
        "O que é buffer? O buffer, neste contexto, é uma espécie de \"banco de dados temporário\" onde as imagens de treinamento são armazenadas antes de serem enviadas para o modelo de inteligência artificial. Configurar o tamanho do buffer ajuda a garantir que os dados sejam processados de forma eficiente e aleatória.\n",
        "\n",
        "O que é batch size? Batch size (tamanho do lote) refere-se ao número de imagens que o modelo processa de uma vez antes de atualizar seus \"conhecimentos\". Em vez de olhar para todas as 60.000 imagens de uma vez (o que seria muito lento e difícil), o modelo processa pequenos lotes de imagens, um lote de cada vez, o que torna o treinamento mais gerenciável e eficiente.\n",
        "\n",
        "O que o código faz?\n",
        "BUFFER_SIZE = 60000: Isso define o tamanho do buffer como 60.000, que é o número total de imagens de treinamento. Isso significa que todas as imagens serão armazenadas no buffer, permitindo que o modelo as processe de maneira embaralhada (aleatória) a cada ciclo de treinamento.\n",
        "\n",
        "BATCH_SIZE = 256: Isso define o tamanho do lote como 256, o que significa que o modelo vai olhar para 256 imagens de cada vez antes de atualizar seus parâmetros. Esse tamanho é um bom equilíbrio entre velocidade e estabilidade durante o treinamento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-4LT2xZ_t0Uh"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp8Rk-T_uOU7"
      },
      "source": [
        "### Criando o dataset de treino\n",
        "\n",
        "O que é um dataset de treino? Um dataset de treino é um conjunto de dados que usamos para \"ensinar\" um modelo de inteligência artificial. Neste caso, estamos usando as imagens do MNIST para treinar nosso modelo.\n",
        "\n",
        "O que o código faz?\n",
        "\n",
        "from_tensor_slices: Essa parte pega todas as imagens de treinamento e as organiza em um formato que o TensorFlow consegue entender.\n",
        "shuffle: Embaralha as imagens de forma aleatória. Isso é importante porque queremos que o modelo aprenda de maneira equilibrada, sem seguir uma ordem específica das imagens.\n",
        "\n",
        "batch: Agrupa as imagens em lotes de tamanho BATCH_SIZE (256 imagens por lote, conforme definimos anteriormente). Isso significa que o modelo processará 256 imagens de cada vez durante o treinamento, o que ajuda a acelerar o processo.\n",
        "\n",
        "Analogia: Imagine que você tem um baralho de cartas. shuffle é como embaralhar o baralho, e batch é como dividir o baralho em pequenos grupos de cartas para facilitar o jogo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SCH_ZdHRudia"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxQ-Gyb0ug-i"
      },
      "source": [
        "### Construção da Rede Geradora\n",
        "\n",
        "O que é a Rede Geradora? A rede geradora é uma parte da GAN que cria novas imagens a partir de ruído aleatório. Ela tenta gerar imagens que sejam tão realistas que enganem a outra parte da GAN, a rede discriminadora.\n",
        "\n",
        "O que o código faz?\n",
        "\n",
        "model = tf.keras.Sequential(): Estamos criando um modelo sequencial, o que significa que as camadas da rede serão empilhadas uma após a outra.\n",
        "Dense: A primeira camada Dense pega um vetor de entrada de 100 números (ruído) e o transforma em um vetor maior (7x7x256). Isso é como pegar uma ideia vaga e expandi-la em algo mais concreto.\n",
        "\n",
        "BatchNormalization: Normaliza os valores dentro da rede, ajudando o modelo a treinar de forma mais eficiente.\n",
        "LeakyReLU: É uma função de ativação que permite que pequenas quantidades de informação negativa passem, o que ajuda a evitar problemas durante o treinamento.\n",
        "\n",
        "Reshape: Esta camada reorganiza os dados para que fiquem no formato de uma imagem pequena (7x7 pixels, com 256 camadas).\n",
        "Conv2DTranspose: Estas camadas aumentam a resolução da imagem, expandindo-a de 7x7 para 14x14, e depois para 28x28, até que a imagem gerada tenha o tamanho final desejado. Pense nisso como uma série de etapas para \"refinar\" e \"ampliar\" a imagem.\n",
        "\n",
        "activation='tanh': A função tanh ajusta os valores de saída para o intervalo [-1, 1], que corresponde ao intervalo em que normalizamos as imagens de treinamento.\n",
        "\n",
        "Analogia: Imagine que a rede geradora é como um artista que começa com uma ideia vaga (ruído) e, aos poucos, vai detalhando essa ideia até que ela se transforme em uma imagem realista."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "3uxX95Ner36j",
        "outputId": "79282e1d-d2cf-437c-aa15-441d0666854e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/carloslessa/FCD/POSTECH/modulo4/04-GenerativeIA/venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,254,400</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,176</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_transpose                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">819,200</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_transpose_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,800</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_transpose_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12544\u001b[0m)          │     \u001b[38;5;34m1,254,400\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12544\u001b[0m)          │        \u001b[38;5;34m50,176\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12544\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_transpose                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │       \u001b[38;5;34m819,200\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_transpose_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │       \u001b[38;5;34m204,800\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_transpose_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │         \u001b[38;5;34m1,600\u001b[0m │\n",
              "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,330,944</span> (8.89 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,330,944\u001b[0m (8.89 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,305,472</span> (8.79 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,305,472\u001b[0m (8.79 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,472</span> (99.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m25,472\u001b[0m (99.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define uma função que constrói o Gerador (DCGAN) para imagens 28×28×1\n",
        "def make_generator_model():\n",
        "    # Modelo sequencial do Keras (empilha camadas em ordem)\n",
        "    model = tf.keras.Sequential()\n",
        "    # Projeção do vetor latente z∈ℝ^{100} para 7×7×256\n",
        "    # (sem bias por estabilidade com BN)\n",
        "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
        "    # Normalização em lote para estabilizar o treinamento e acelerar a convergência\n",
        "    model.add(layers.BatchNormalization())\n",
        "    # Ativação LeakyReLU para evitar \"dying ReLU\" e manter gradientes fluindo\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # Remodela o vetor [7*7*256] para um mapa de ativação [7, 7, 256]\n",
        "    model.add(layers.Reshape((7, 7, 256)))\n",
        "    # Upsample #1: mantém 7×7, reduz canais 256→128 com kernel 5×5 e stride 1\n",
        "    # (deconvolução)\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    # BN para estabilizar estatísticas após a deconvolução\n",
        "    model.add(layers.BatchNormalization())\n",
        "    # LeakyReLU para não saturar e manter gradiente\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # Upsample #2: dobra a resolução 7×7→14×14, reduz canais 128→64\n",
        "    # (kernel 5×5, stride 2)\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    # BN novamente para estabilizar\n",
        "    model.add(layers.BatchNormalization())\n",
        "    # LeakyReLU na saída intermediária\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # Upsample #3 (saída): 14×14→28×28 e mapeia canais 64→1;\n",
        "    # tanh gera pixels em [-1, 1]\n",
        "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "\n",
        "    # Retorna o gerador pronto para treinar/inferir\n",
        "    return model\n",
        "\n",
        "# Cria uma instância do gerador usando a função definida acima\n",
        "generator = make_generator_model()\n",
        "# Exibe o resumo da arquitetura (camadas, shapes e nº de parâmetros)\n",
        "generator.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywROs42Uu185"
      },
      "source": [
        "### Construção da Rede Discriminadora\n",
        "\n",
        "Função da Rede Discriminadora: A rede discriminadora é uma parte fundamental da GAN (Generative Adversarial Network). Ela funciona como um \"crítico\" que avalia se uma imagem é real (do conjunto de dados de treino) ou falsa (gerada pela rede geradora). Seu objetivo é distinguir imagens reais das falsas com a maior precisão possível.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8huufu9vAEQ"
      },
      "source": [
        "Modelo Sequencial: Assim como na rede geradora, aqui também estamos criando um modelo sequencial (Sequential), o que significa que as camadas da rede serão empilhadas uma após a outra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "2JYPbLUGuzsg",
        "outputId": "75bb4cd3-570b-4bc9-f2f7-057a57d0b7f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/carloslessa/FCD/POSTECH/modulo4/04-GenerativeIA/venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,664</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6272</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,273</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m1,664\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │       \u001b[38;5;34m204,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ leaky_re_lu_4 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6272\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m6,273\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">212,865</span> (831.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m212,865\u001b[0m (831.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">212,865</span> (831.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m212,865\u001b[0m (831.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define o Discriminador (DCGAN) que classifica 28×28×1 como real/falso\n",
        "def make_discriminator_model():\n",
        "    # Modelo sequencial do Keras (camadas empilhadas)\n",
        "    model = tf.keras.Sequential()\n",
        "    # Bloco conv #1: 64 filtros, kernel 5×5, stride 2, mantém spatial com 'same'\n",
        "    # Input explícito: imagens 28×28×1 (MNIST)\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n",
        "    # Ativação LeakyReLU (negative_slope≈0.3 por padrão no Keras)\n",
        "    model.add(layers.LeakyReLU())\n",
        "    # Dropout para regularizar e reduzir overfitting\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    # Bloco conv #2: aprofunda para 128 filtros, reduz mais a resolução\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    # LeakyReLU mantém gradientes em regiões negativas\n",
        "    model.add(layers.LeakyReLU())\n",
        "    # Dropout adicional após a ativação\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    # Achata mapas de ativação para vetor\n",
        "    model.add(layers.Flatten())\n",
        "    # Camada densa final: 1 logit (sem sigmoid) → usar loss com from_logits=True\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    # Retorna o discriminador pronto\n",
        "    return model\n",
        "\n",
        "# Instancia o discriminador usando a função acima\n",
        "discriminator = make_discriminator_model()\n",
        "# Mostra o resumo da arquitetura (shapes e nº de parâmetros)\n",
        "discriminator.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KwANTI8vWlO"
      },
      "source": [
        "A Rede Discriminadora é como um crítico rigoroso que analisa imagens e decide se elas parecem reais ou falsas.\n",
        "\n",
        "Ela usa várias camadas convolucionais para \"entender\" diferentes aspectos das imagens, e então, no final, decide se a imagem deve ser considerada real ou falsa com base em todos esses aspectos.\n",
        "\n",
        "O código configura essa rede e a prepara para ser usada junto com a rede geradora no treinamento da GAN, onde as duas redes competirão para melhorar constantemente.\n",
        "\n",
        "Analogia:\n",
        "Pense na Rede Discriminadora como um avaliador de diamantes. Ele examina cada diamante (imagem) cuidadosamente, procurando por sinais que indiquem se o diamante é verdadeiro (imagem real) ou falso (imagem gerada). Ele usa várias ferramentas (camadas convolucionais) para fazer essa avaliação e, no final, dá um veredicto (0 ou 1)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO+Cq4C7RcP4Tq+NzcThUl8",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
