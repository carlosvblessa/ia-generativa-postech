{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WVkEp4RNEmYQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVbQPdAY8UBA"
      },
      "source": [
        "Objetivo: A função softmax converte uma lista de números em uma distribuição de probabilidades.\n",
        "Como funciona:\n",
        "Primeiro, np.exp(x) calcula o exponencial de cada valor em x.\n",
        "Em seguida, np.sum(np.exp(x), axis=-1, keepdims=True) calcula a soma desses exponenciais ao longo da última dimensão (axis=-1), mantendo as dimensões originais (keepdims=True).\n",
        "Finalmente, cada valor exponencial é dividido pela soma total para obter a probabilidade correspondente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XbEBzCfn8IUs"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww64pt5L8ZBL"
      },
      "source": [
        "Objetivo: A função cross_attention implementa o mecanismo de atenção cruzada, calculando as \"pesos de atenção\" e a \"saída ponderada\" para as sequências de entrada.\n",
        "Passos detalhados:\n",
        "\n",
        "scores = np.dot(Q, K.T): Calcula as pontuações de atenção multiplicando as matrizes Q (queries) e K.T (a transposta de keys). Isso mede a similaridade entre cada par de elementos de Q e K.\n",
        "\n",
        "attention_weights = softmax(scores): Aplica a função softmax nas pontuações para transformar essas pontuações em probabilidades, que indicam a importância relativa de cada palavra em K para cada palavra em Q.\n",
        "\n",
        "output = np.dot(attention_weights, V): Multiplica os pesos de atenção pelos valores V para calcular a saída final, que é uma combinação ponderada dos valores V baseada nas probabilidades calculadas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mPHe5Gjj8fl6"
      },
      "outputs": [],
      "source": [
        "def cross_attention(Q, K, V):\n",
        "    scores = np.dot(Q, K.T)\n",
        "    attention_weights = softmax(scores)\n",
        "    output = np.dot(attention_weights, V)\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "26Dm7BOM8LuR"
      },
      "outputs": [],
      "source": [
        "Q = np.array([\n",
        "    [1, 0, 0],\n",
        "    [0, 1, 0]\n",
        "])\n",
        "\n",
        "K = np.array([\n",
        "    [1, 0, 1],\n",
        "    [0, 2, 0],\n",
        "    [1, 1, 1]\n",
        "])\n",
        "V = K\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiiG05wi8jVH"
      },
      "source": [
        "Q: É uma matriz que representa a sequência de destino (ou queries). Cada linha corresponde a uma \"palavra\" na sequência de destino.\n",
        "\n",
        "K: É a matriz que representa a sequência de origem (ou keys). Cada linha corresponde a uma \"palavra\" na sequência de origem.\n",
        "\n",
        "V: Geralmente, V (values) é igual a K. Representa os valores que serão combinados para gerar a saída."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T3x9PFh58u2x"
      },
      "outputs": [],
      "source": [
        "output, attention_weights = cross_attention(Q, K, V)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfUS8xiq8we_"
      },
      "source": [
        "Chama a função: A função cross_attention é chamada com as matrizes Q, K, e V.\n",
        "Resultados:\n",
        "\n",
        "output: É a saída final, ou seja, a nova representação das palavras na sequência de destino depois de aplicar a atenção cruzada.\n",
        "\n",
        "attention_weights: São os pesos de atenção, que mostram quanto cada palavra na sequência de destino se relaciona com as palavras na sequência de origem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ST-nBBw8NEs",
        "outputId": "99c79e3c-3b1b-44a2-f75a-7d2e51cad7b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequência de Destino (Q):\n",
            "[[1 0 0]\n",
            " [0 1 0]]\n",
            "\n",
            "Sequência de Origem (K e V):\n",
            "[[1 0 1]\n",
            " [0 2 0]\n",
            " [1 1 1]]\n",
            "\n",
            "Pesos de Atenção:\n",
            "[[0.4223188  0.1553624  0.4223188 ]\n",
            " [0.09003057 0.66524096 0.24472847]]\n",
            "\n",
            "Saída com Cross-Attention Aplicada:\n",
            "[[0.8446376  0.73304361 0.8446376 ]\n",
            " [0.33475904 1.57521038 0.33475904]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Sequência de Destino (Q):\")\n",
        "print(Q)\n",
        "print(\"\\nSequência de Origem (K e V):\")\n",
        "print(K)\n",
        "print(\"\\nPesos de Atenção:\")\n",
        "print(attention_weights)\n",
        "print(\"\\nSaída com Cross-Attention Aplicada:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goZAQ9p385fs"
      },
      "source": [
        "Objetivo: Exibir os valores de Q, K, os pesos de atenção calculados, e a saída final output.\n",
        "Resultados mostrados:\n",
        "\n",
        "Sequência de Destino (Q): Exibe a matriz Q.\n",
        "\n",
        "Sequência de Origem (K e V): Exibe a matriz K (e V que é igual a K).\n",
        "Pesos de Atenção: Mostra as probabilidades calculadas que indicam a importância de cada palavra.\n",
        "\n",
        "Saída com Cross-Attention Aplicada: Mostra a nova sequência gerada após a aplicação da atenção cruzada."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMEGtCYqhjz8JuaUvuX1ygp",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
